{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of Object Detection Models based on Detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Development Environment\n",
    "1. Environments\n",
    "    * GPU: RTX3070 8GB\n",
    "    * OS: Ubuntu20.04\n",
    "    * CUDA: 11.3\n",
    "    * Pytorch==1.10\n",
    "    * Detectron2==0.6\n",
    "\n",
    "2. Installation\n",
    "    ```bash\n",
    "    sudo apt-get install -y python3-dev python3-venv\n",
    "    python3 -m venv env\n",
    "    source env/bin/activate\n",
    "    python -m pip install pip -U\n",
    "    python -m pip install -r requirements.txt\n",
    "    python -m ipykernel install --user --name env --display-name ensemble_detectron2\n",
    "    python -m pip install \"git+https://github.com/facebookresearch/detectron2@v0.6\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip val2017.zip\n",
    "!rm val2017.zip\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip\n",
    "!rm annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation.coco_evaluation import COCOEvaluator\n",
    "from detectron2.data.build import build_detection_test_loader\n",
    "from detectron2.evaluation.evaluator import inference_on_dataset\n",
    "from detectron2.evaluation.evaluator import inference_context\n",
    "from detectron2.structures import Instances, Boxes\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import ensemble_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Models, Data and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"dataset_val\", {}, \"./annotations/instances_val2017.json\", \"./val2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /home/kyungpyo/git/Ensemble-Object-Detection-using-Detectron2/env/lib/python3.8/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mpixel_mean\u001b[0m\n",
      "  \u001b[35mpixel_std\u001b[0m\n",
      "\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_configs = [\n",
    "    \"faster_rcnn_R_50_C4_1x.yaml\",\n",
    "    \"faster_rcnn_R_50_DC5_1x.yaml\",\n",
    "    \"retinanet_R_50_FPN_1x.yaml\",\n",
    "]\n",
    "\n",
    "models = dict()\n",
    "for config in model_configs:\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{config}\"))\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{config}\")\n",
    "    cfg.DATASETS.VAL = (\"dataset_val\",)\n",
    "\n",
    "    models[config] = DefaultPredictor(cfg).model\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.DATASETS.VAL = (\"dataset_val\",)\n",
    "val_loader = build_detection_test_loader(cfg, \"dataset_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/kyungpyo/git/Ensemble-Object-Detection-using-Detectron2/env/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 5000/5000 [10:35<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "\n",
      "faster_rcnn_R_50_C4_1x.yaml\n",
      "\n",
      "================================================================\n",
      "\n",
      "Loading and preparing results...\n",
      "DONE (t=0.92s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.357\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.561\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.380\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.192\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.409\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.485\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.506\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.310\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.563\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.663\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:10<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "\n",
      "faster_rcnn_R_50_DC5_1x.yaml\n",
      "\n",
      "================================================================\n",
      "\n",
      "Loading and preparing results...\n",
      "DONE (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.373\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.587\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.397\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.201\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.488\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.511\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.299\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.565\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.670\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/kyungpyo/git/Ensemble-Object-Detection-using-Detectron2/env/lib/python3.8/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "100%|██████████| 5000/5000 [05:09<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "\n",
      "retinanet_R_50_FPN_1x.yaml\n",
      "\n",
      "================================================================\n",
      "\n",
      "Loading and preparing results...\n",
      "DONE (t=0.99s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.567\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.403\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.551\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.372\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.701\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for config, model in models.items():\n",
    "    evaluator = COCOEvaluator(\"dataset_val\", False, output_dir=f\"results/{config.split('.')[0]}\")\n",
    "    evaluator.reset()\n",
    "    with inference_context(model), torch.no_grad():\n",
    "        iter = tqdm(val_loader, total=len(val_loader))\n",
    "        for idx, inputs in enumerate(iter):\n",
    "            outputs = model(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            evaluator.process(inputs, outputs)\n",
    "\n",
    "    print(\"\\n================================================================\\n\")\n",
    "    print(config)\n",
    "    print(\"\\n================================================================\\n\")\n",
    "    results = evaluator.evaluate()\n",
    "    print(\"\\n================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box mAP of Baseline Models\n",
    "\n",
    "| Baselines                   | Box AP @(IoU=0.50:0.95, area=all, maxDets=100) |\n",
    "|-----------------------------|------|\n",
    "|faster_rcnn_R_50_C4_1x.yaml  | 0.357|\n",
    "|faster_rcnn_R_50_DC5_1x.yaml | 0.373|\n",
    "|retinanet_R_50_FPN_1x.yaml   | 0.374|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble using Weighted Boxes Fusion Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Detection Results and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.62s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.33s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=2.95s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "gt_path = \"./annotations/instances_val2017.json\"\n",
    "coco_gt = COCO(gt_path)\n",
    "\n",
    "dt_paths = [\n",
    "    \"./results/faster_rcnn_R_50_C4_1x/coco_instances_results.json\",\n",
    "    \"./results/faster_rcnn_R_50_DC5_1x/coco_instances_results.json\",\n",
    "    \"./results/retinanet_R_50_FPN_1x/coco_instances_results.json\",\n",
    "]\n",
    "\n",
    "coco_dts = [coco_gt.loadRes(dt_path) for dt_path in dt_paths]\n",
    "img_ids = coco_gt.getImgIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "\n",
      "Ensemble using Weighted Boxes Fusion\n",
      "iou thr: 0.7, skip_box_thr: 0.0001\n",
      "\n",
      "================================================================\n",
      "\n",
      "Loading and preparing results...\n",
      "DONE (t=1.45s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=40.65s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=8.14s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.601\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.443\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.243\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.453\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.334\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.545\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.588\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.638\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.737\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "iou_thr = 0.7\n",
    "skip_box_thr = 0.0001\n",
    "\n",
    "print(\"\\n================================================================\\n\")\n",
    "print(\"Ensemble using Weighted Boxes Fusion\")\n",
    "print(f\"iou thr: {iou_thr}, skip_box_thr: {skip_box_thr}\")\n",
    "print(\"\\n================================================================\\n\")\n",
    "\n",
    "ensemble = []\n",
    "cnt_id = 0\n",
    "for img_id in img_ids:\n",
    "    height = float(coco_gt.loadImgs(img_id)[0][\"height\"])\n",
    "    width = float(coco_gt.loadImgs(img_id)[0][\"width\"])\n",
    "\n",
    "    tmp_anns = []\n",
    "    boxes_list = []\n",
    "    scores_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for coco_dt in coco_dts:\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        labels = []\n",
    "        for ann in coco_dt.imgToAnns[img_id]:\n",
    "            x1, y1 = ann[\"bbox\"][0], ann[\"bbox\"][1]\n",
    "            x2 = ann[\"bbox\"][0] + ann[\"bbox\"][2]\n",
    "            y2 = ann[\"bbox\"][1] + ann[\"bbox\"][3]\n",
    "            x1, x2 = x1/width, x2/width\n",
    "            y1, y2 = y1/height, y2/height\n",
    "\n",
    "            x1 = min(1.000, max(0.000, x1))\n",
    "            x2 = min(1.000, max(0.000, x2))\n",
    "            y1 = min(1.000, max(0.000, y1))\n",
    "            y2 = min(1.000, max(0.000, y2))\n",
    "                \n",
    "            boxes.append([x1,y1,x2,y2])\n",
    "            scores.append(ann[\"score\"])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        boxes_list.append(boxes)\n",
    "        scores_list.append(scores)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    boxes, scores, labels = ensemble_boxes.weighted_boxes_fusion(\n",
    "                                            boxes_list, \n",
    "                                            scores_list, \n",
    "                                            labels_list, \n",
    "                                            weights=None, \n",
    "                                            iou_thr=iou_thr, \n",
    "                                            skip_box_thr=skip_box_thr)\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        x1 *= width\n",
    "        x2 *= width\n",
    "        y1 *= height\n",
    "        y2 *= height\n",
    "\n",
    "        ann = dict(\n",
    "            image_id=img_id,\n",
    "            category_id=label,\n",
    "            bbox=[x1, y1, x2-x1, y2-y1],\n",
    "            score=score,\n",
    "            id=cnt_id,\n",
    "        )\n",
    "\n",
    "        ensemble.append(ann)\n",
    "        \n",
    "        cnt_id += 1\n",
    "\n",
    "coco_ensemble = coco_gt.loadRes(ensemble)\n",
    "coco_eval = COCOeval(coco_gt, coco_ensemble, \"bbox\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "print(\"\\n================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Models                   | Box AP @(IoU=0.50:0.95, area=all, maxDets=100) |\n",
    "|-----------------------------|------|\n",
    "|faster_rcnn_R_50_C4_1x.yaml  | 0.357|\n",
    "|faster_rcnn_R_50_DC5_1x.yaml | 0.373|\n",
    "|retinanet_R_50_FPN_1x.yaml   | 0.374|\n",
    "|__Ensemble Model__   | __*0.403 (+0.029)*__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a7cd6bd6adba24856cb99037131169106d508c2430b6d3766fea104d49690de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
